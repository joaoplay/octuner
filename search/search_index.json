{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#octuner-multi-provider-llm-optimizer","title":"Octuner - Multi-Provider LLM Optimizer","text":"<p>Optimize LLM providers, models, and parameters \u2014 without the guesswork.</p> <p>Octuner is a lightweight library that solves the decision-making process when integrating with LLMs, especially in multi-step model chaining scenarios.</p>"},{"location":"#why-octuner","title":"Why Octuner?","text":"<p>Building LLM applications often feels like solving a puzzle:</p> <ul> <li>Which provider? OpenAI, Gemini, Anthropic\u2026 or self-hosted (Ollama, vLLM, etc.)?  </li> <li>Which model? GPT-4o, Gemini Pro, Claude\u2026?  </li> <li>Which parameters? Temperature, top-p, max_tokens\u2026?  </li> <li>How to balance quality, cost, and latency?</li> </ul> <p>Things get harder with model chaining, where each step depends on the previous one:</p> <pre><code>Input \u2192 [LLM A] \u2192 Intermediate Result \u2192 [LLM B] \u2192 Final Output\n</code></pre> <p>Manual trial-and-error leads to inconsistent performance, wasted budget, and provider lock-in. Octuner removes the guesswork.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Build a tiny sentiment chain that first explains why, then outputs a single-word label. You\u2019ll pass an explicit YAML config path so it\u2019s ready for optimization.</p>"},{"location":"#1-create-your-model-chain","title":"1. Create your model chain","text":"<pre><code>from octuner import MultiProviderTunableLLM\n\nclass SentimentChain:\n    def __init__(self, config_file: str):\n        # Reason step (clear explanation)\n        self.reasoner = MultiProviderTunableLLM(\n            config_file,\n            default_provider=\"openai\",\n            default_model=\"gpt-4o-mini\",\n        )\n        # Label step (concise single-word output)\n        self.labeler = MultiProviderTunableLLM(\n            config_file,\n            default_provider=\"gemini\",\n            default_model=\"gemini-1.5-flash\",\n        )\n\n    def _build_reason_prompt(self, text: str) -&gt; str:\n        return (\n            \"Explain the sentiment (positive/negative/neutral) of the text below. \"\n            \"Keep the reasoning short and specific.\\n\\n\"\n            f\"Text: {text}\\n\"\n        )\n\n    def _build_label_prompt(self, reasoning: str) -&gt; str:\n        return (\n            \"Given the reasoning below, respond with only one word: \"\n            \"positive | negative | neutral.\\n\\n\"\n            f\"Reasoning:\\n{reasoning}\\n\"\n        )\n\n    def predict(self, text: str) -&gt; dict:\n        reason = self.reasoner.call(self._build_reason_prompt(text)).text\n        label = self.labeler.call(self._build_label_prompt(reason)).text.strip().lower()\n        return {\"sentiment\": label, \"why\": reason}\n</code></pre>"},{"location":"#2-add-a-dataset-and-metric","title":"2. Add a dataset and metric","text":"<pre><code>dataset = [\n    {\"input\": \"I love this!\", \"target\": {\"sentiment\": \"positive\"}},\n    {\"input\": \"This is awful.\", \"target\": {\"sentiment\": \"negative\"}},\n    {\"input\": \"It's fine.\", \"target\": {\"sentiment\": \"neutral\"}},\n]\n\ndef metric(output, target):\n    return 1.0 if output[\"sentiment\"] == target[\"sentiment\"] else 0.0\n</code></pre>"},{"location":"#3-optimize","title":"3. Optimize","text":"<pre><code>from octuner import AutoTuner, apply_best\n\nchain = SentimentChain(\"configs/llm.yaml\")  # explicit YAML config path\n\ntuner = AutoTuner.from_component(\n    component=chain,\n    entrypoint=lambda c, x: c.predict(x),\n    dataset=dataset,\n    metric=metric,\n)\n\n# Focus on the most impactful knobs first\ntuner.include([\n    \"reasoner.provider_model\", \"reasoner.temperature\",\n    \"labeler.provider_model\", \"labeler.temperature\",\n])\n\nresult = tuner.search(max_trials=12, mode=\"pareto\")\nresult.save_best(\"optimized_sentiment_chain.yaml\")\n\napply_best(chain, \"optimized_sentiment_chain.yaml\")\nprint(chain.predict(\"The new UI is a joy to use.\"))\n</code></pre>"},{"location":"#full-example","title":"Full example","text":"<pre><code>from octuner import MultiProviderTunableLLM, AutoTuner, apply_best\n\nclass SentimentChain:\n    def __init__(self, config_file: str):\n        self.reasoner = MultiProviderTunableLLM(\n            config_file,\n            default_provider=\"openai\",\n            default_model=\"gpt-4o-mini\",\n        )\n        self.labeler = MultiProviderTunableLLM(\n            config_file,\n            default_provider=\"gemini\",\n            default_model=\"gemini-1.5-flash\",\n        )\n\n    def _build_reason_prompt(self, text: str) -&gt; str:\n        return (\n            \"Explain the sentiment (positive/negative/neutral) of the text below. \"\n            \"Keep the reasoning short and specific.\\n\\n\"\n            f\"Text: {text}\\n\"\n        )\n\n    def _build_label_prompt(self, reasoning: str) -&gt; str:\n        return (\n            \"Given the reasoning below, respond with only one word: \"\n            \"positive | negative | neutral.\\n\\n\"\n            f\"Reasoning:\\n{reasoning}\\n\"\n        )\n\n    def predict(self, text: str) -&gt; dict:\n        reason = self.reasoner.call(self._build_reason_prompt(text)).text\n        label = self.labeler.call(self._build_label_prompt(reason)).text.strip().lower()\n        return {\"sentiment\": label, \"why\": reason}\n\ndataset = [\n    {\"input\": \"I love this!\", \"target\": {\"sentiment\": \"positive\"}},\n    {\"input\": \"This is awful.\", \"target\": {\"sentiment\": \"negative\"}},\n    {\"input\": \"It's fine.\", \"target\": {\"sentiment\": \"neutral\"}},\n]\n\ndef metric(output, target):\n    return 1.0 if output[\"sentiment\"] == target[\"sentiment\"] else 0.0\n\nchain = SentimentChain(\"configs/llm.yaml\")\n\ntuner = AutoTuner.from_component(\n    component=chain,\n    entrypoint=lambda c, x: c.predict(x),\n    dataset=dataset,\n    metric=metric,\n)\n\ntuner.include([\n    \"reasoner.provider_model\", \"reasoner.temperature\",\n    \"labeler.provider_model\", \"labeler.temperature\",\n])\n\nresult = tuner.search(max_trials=12, mode=\"pareto\")\nresult.save_best(\"optimized_sentiment_chain.yaml\")\n\napply_best(chain, \"optimized_sentiment_chain.yaml\")\nprint(chain.predict(\"The new UI is a joy to use.\"))\n</code></pre>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#multi-provider-optimization","title":"Multi-Provider Optimization","text":"<p>Automatically discover the best combination of:</p> <ul> <li>Providers: OpenAI, Gemini, Anthropic, or self-hosted (Ollama, vLLM, etc.)</li> <li>Models: GPT-4o, Gemini Pro, Claude, or self-hosted (Llama, Mistral, etc.)</li> <li>Parameters: temperature, top_p, max_tokens, web search</li> <li>Capabilities: Web search, etc.</li> </ul>"},{"location":"#multiple-optimization-modes","title":"Multiple Optimization Modes","text":"<ul> <li>Pareto: Balance quality, cost, and latency (default)</li> <li>Constrained: Maximize quality within cost/latency limits</li> <li>Scalarized: Optimize weighted combination of metrics</li> <li>Quality-focused: Maximize performance regardless of cost/time</li> <li>Cost-focused: Minimize spending while meeting quality thresholds</li> <li>Speed-focused: Optimize for fastest response within quality bounds</li> </ul>"},{"location":"#flexible-parameter-control","title":"Flexible Parameter Control","text":"<pre><code>providers:\n  openai:\n    model_capabilities:\n      gpt-4o-mini:\n        supported_parameters: [temperature, top_p, max_tokens]\n        parameter_ranges:\n          temperature: [0.0, 2.0]\n          max_tokens: [50, 4000]\n        default_parameters:\n          temperature: 0.7\n          max_tokens: 1000\n</code></pre>"},{"location":"#web-search-integration","title":"Web Search Integration","text":"<ul> <li>OpenAI: Built-in web search capabilities</li> <li>Gemini: Native Google grounding tool for web context</li> <li>Tunable: Let optimization decide when web search improves performance</li> </ul>"},{"location":"#learn-more","title":"Learn More","text":"<ol> <li>Installation &amp; Setup - Get started quickly</li> <li>Getting Started - Complete workflow, examples, and custom provider setup  </li> <li>API Reference - Complete API documentation</li> <li>Contributing - How to contribute to Octuner</li> </ol> <p>Octuner helps developers build better LLM applications by systematically optimizing the quality vs cost vs time triangle through explicit configuration management and data-driven parameter tuning.</p>"},{"location":"contributing/","title":"Contributing to Octuner","text":"<p>Thank you for your interest in contributing to Octuner! This guide will help you get started.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":""},{"location":"contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork locally:</li> </ol> <pre><code>git clone https://github.com/YOUR-USERNAME/octuner.git\ncd octuner\n</code></pre>"},{"location":"contributing/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<p>Octuner uses <code>uv</code> for dependency management. If you don't have it installed:</p> <pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n</code></pre> <p>Then install the project dependencies:</p> <pre><code># Install all dependencies including dev dependencies\nuv sync\n</code></pre>"},{"location":"contributing/#3-create-a-branch","title":"3. Create a Branch","text":"<p>Create a new branch for your changes:</p> <pre><code>git checkout -b feature/your-feature-name\n</code></pre> <p>Use descriptive branch names:</p> <ul> <li><code>feature/add-anthropic-provider</code> for new features</li> <li><code>fix/temperature-validation</code> for bug fixes</li> <li><code>docs/improve-quickstart</code> for documentation</li> </ul>"},{"location":"contributing/#making-changes","title":"Making Changes","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<p>Octuner includes a convenient test runner script that makes it easy to run different types of tests. Always run tests before submitting a PR!</p>"},{"location":"contributing/#basic-usage","title":"Basic Usage","text":"<pre><code># Check test environment is properly set up\npython run_tests.py check\n\n# Run all tests\npython run_tests.py all\n\n# Run only unit tests\npython run_tests.py unit\n\n# Run only integration tests\npython run_tests.py integration\n\n# Run fast tests (excludes slow/external tests)\npython run_tests.py fast\n</code></pre>"},{"location":"contributing/#advanced-options","title":"Advanced Options","text":"<pre><code># Run tests with coverage report\npython run_tests.py unit --coverage\npython run_tests.py all --coverage\n\n# Run tests with verbose output\npython run_tests.py all --verbose\n\n# Run a specific test file or function\npython run_tests.py specific --test-path tests/unit/test_providers/test_openai.py\npython run_tests.py specific --test-path tests/unit/test_providers/test_openai.py::TestOpenAIProvider::test_call\n</code></pre>"},{"location":"contributing/#adding-tests","title":"Adding Tests","text":"<ul> <li>Add unit tests for new functionality in <code>tests/unit/</code></li> <li>Add integration tests for end-to-end scenarios in <code>tests/integration/</code></li> </ul>"},{"location":"contributing/#submitting-a-pull-request","title":"Submitting a Pull Request","text":""},{"location":"contributing/#1-commit-your-changes","title":"1. Commit Your Changes","text":"<p>Write clear, concise commit messages:</p> <pre><code>git add .\ngit commit -m \"Add Anthropic provider support\"\n</code></pre> <p>Good commit message examples: - <code>Add support for Anthropic Claude models</code> - <code>Fix temperature parameter validation</code> - <code>Update getting started guide with new examples</code></p>"},{"location":"contributing/#2-push-to-your-fork","title":"2. Push to Your Fork","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre>"},{"location":"contributing/#3-create-a-pull-request","title":"3. Create a Pull Request","text":"<ol> <li>Go to the Octuner repository</li> <li>Click \"New Pull Request\"</li> <li>Select your fork and branch</li> <li>Fill in the PR template with:</li> <li>Description: What does this PR do?</li> <li>Changes: List of main changes</li> <li>Testing: How did you test this?</li> <li>Related Issues: Link any related issues</li> </ol>"},{"location":"contributing/#development-guidelines","title":"Development Guidelines","text":""},{"location":"contributing/#adding-a-new-provider","title":"Adding a New Provider","text":"<p>To add a new LLM provider:</p> <ol> <li>Create a new file in <code>octuner/providers/</code> (e.g., <code>anthropic.py</code>)</li> <li>Inherit from <code>BaseLLMProvider</code></li> <li>Implement required methods: <code>call()</code>, <code>configure()</code>, etc.</li> <li>Register the provider in <code>octuner/providers/registry.py</code></li> <li>Add tests in <code>tests/unit/test_providers/</code></li> <li>Add a config template in <code>config_templates/</code></li> </ol> <p>Example structure:</p> <pre><code>from octuner.providers.base import BaseLLMProvider\n\nclass AnthropicProvider(BaseLLMProvider):\n    def call(self, prompt: str, **kwargs):\n        # Implementation\n        pass\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update relevant documentation in <code>docs/</code></li> <li>Add docstrings to all public functions and classes</li> <li>Follow Google-style docstring format</li> <li>Update <code>docs/reference.md</code> if you add new public APIs</li> </ul>"},{"location":"contributing/#configuration-files","title":"Configuration Files","text":"<ul> <li>Test with different config templates</li> <li>Ensure backward compatibility</li> <li>Document any new configuration options</li> </ul>"},{"location":"contributing/#questions","title":"Questions?","text":"<ul> <li>Issues: Open an issue for bugs or feature requests</li> <li>Discussions: Use GitHub Discussions for questions</li> <li>Pull Requests: Check existing PRs to avoid duplication</li> </ul>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help make Octuner better for everyone</li> </ul> <p>Thank you for contributing to Octuner! \ud83c\udf89</p>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Get started with Octuner in 3 simple steps: Configure, Build, Optimize.</p>"},{"location":"getting-started/#quick-example","title":"Quick Example","text":"<pre><code>from octuner import MultiProviderTunableLLM, AutoTuner, apply_best\n\n# 1. Create a tunable LLM\nllm = MultiProviderTunableLLM(config_file=\"config_templates/openai_basic.yaml\")\n\n# 2. Use it\nresponse = llm.call(\"What is the capital of France?\")\nprint(response.text)\n\n# 3. Optimize it (when you have a dataset)\ndataset = [\n    {\"input\": \"What is 2+2?\", \"target\": \"4\"},\n    {\"input\": \"What is 3+3?\", \"target\": \"6\"},\n]\n\ntuner = AutoTuner(\n    component=llm,\n    entrypoint=lambda llm, inp: llm.call(inp).text,\n    dataset=dataset,\n    metric=lambda output, target: 1.0 if target in output else 0.0\n)\n\nresult = tuner.search(n_trials=10)\nresult.save_best(\"optimized_config.yaml\")\n</code></pre>"},{"location":"getting-started/#1-copy-a-config-template","title":"1. Copy a Config Template","text":"<p>Octuner uses explicit YAML configuration files. Start by copying a template:</p> <pre><code># For OpenAI only\ncp config_templates/openai_basic.yaml my_config.yaml\n\n# For multiple providers\ncp config_templates/multi_provider.yaml my_config.yaml\n</code></pre> <p>Set your API keys:</p> <pre><code>export OPENAI_API_KEY=\"sk-your-key\"\nexport GOOGLE_API_KEY=\"your-key\"  # if using Gemini\n</code></pre>"},{"location":"getting-started/#2-build-your-component","title":"2. Build Your Component","text":"<p>Create a simple component that uses the tunable LLM:</p> <pre><code>from octuner import MultiProviderTunableLLM\n\nclass SentimentAnalyzer:\n    def __init__(self, config_file: str):\n        self.llm = MultiProviderTunableLLM(config_file=config_file)\n\n    def analyze(self, text: str) -&gt; str:\n        prompt = f\"Classify the sentiment (positive/negative/neutral): {text}\"\n        response = self.llm.call(prompt)\n        return response.text.strip().lower()\n\n# Use it\nanalyzer = SentimentAnalyzer(\"my_config.yaml\")\nresult = analyzer.analyze(\"I love this product!\")\nprint(result)  # \"positive\"\n</code></pre>"},{"location":"getting-started/#3-optimize","title":"3. Optimize","text":"<p>When you have a dataset of examples, optimize your component:</p> <pre><code>from octuner import AutoTuner, apply_best\n\n# Prepare dataset\ndataset = [\n    {\"input\": \"I love this!\", \"target\": \"positive\"},\n    {\"input\": \"This is terrible\", \"target\": \"negative\"},\n    {\"input\": \"It's okay\", \"target\": \"neutral\"},\n]\n\n# Define metric\ndef accuracy_metric(output: str, target: str) -&gt; float:\n    return 1.0 if target in output else 0.0\n\n# Create optimizer\nanalyzer = SentimentAnalyzer(\"my_config.yaml\")\ntuner = AutoTuner(\n    component=analyzer,\n    entrypoint=lambda analyzer, text: analyzer.analyze(text),\n    dataset=dataset,\n    metric=accuracy_metric\n)\n\n# Run optimization\nresult = tuner.search(n_trials=20)\nprint(f\"Best quality: {result.best_trial.metrics.quality:.2f}\")\n\n# Save and apply\nresult.save_best(\"optimized.yaml\")\napply_best(analyzer, \"optimized.yaml\")\n</code></pre> <p>That's it! Your component will now use optimized parameters for provider, model, temperature, and other settings.</p>"},{"location":"getting-started/#what-gets-optimized","title":"What Gets Optimized?","text":"<p>Octuner automatically discovers and tunes:</p> <ul> <li>Provider &amp; Model: OpenAI vs Gemini, GPT-4 vs GPT-3.5, etc.</li> <li>Temperature: Creativity vs consistency</li> <li>Max Tokens: Response length</li> <li>Other parameters: top_p, frequency_penalty, etc.</li> </ul> <p>All defined in your YAML config file.</p>"},{"location":"getting-started/#adding-custom-providers","title":"Adding Custom Providers","text":"<p>Octuner is designed to be extensible. You can add your own custom providers for self-hosted LLMs (like Ollama, vLLM, or LM Studio) or any other LLM service.</p>"},{"location":"getting-started/#how-it-works","title":"How It Works","text":"<ol> <li>Create a Provider Class - Inherit from <code>BaseLLMProvider</code></li> <li>Register the Provider - Use <code>register_provider()</code> function</li> <li>Configure It - Create a YAML config with models and parameters</li> <li>Use It - Reference it in your <code>MultiProviderTunableLLM</code></li> </ol>"},{"location":"getting-started/#step-by-step-example-ollama-provider","title":"Step-by-Step Example: Ollama Provider","text":"<p>Let's create a custom provider for Ollama, a self-hosted LLM runtime.</p>"},{"location":"getting-started/#1-create-the-provider-class","title":"1. Create the Provider Class","text":"<pre><code># my_custom_provider.py\nimport time\nfrom typing import Any, Optional\nimport requests\nfrom octuner.providers import BaseLLMProvider, LLMResponse\n\nclass OllamaProvider(BaseLLMProvider):\n    \"\"\"Custom provider for Ollama self-hosted LLMs.\"\"\"\n\n    def __init__(self, config_loader, **kwargs):\n        super().__init__(config_loader=config_loader, **kwargs)\n        self.provider_name = \"ollama\"\n        self.base_url = kwargs.get('base_url', 'http://localhost:11434')\n\n    def call(self, prompt: str, system_prompt: Optional[str] = None, **kwargs) -&gt; LLMResponse:\n        \"\"\"Make a call to Ollama API.\"\"\"\n        start_time = time.time()\n\n        # Get model from kwargs or config\n        model = self._get_parameter(\"model\", kwargs, \"llama2\")\n\n        # Build request\n        request_data = {\n            \"model\": model,\n            \"prompt\": prompt,\n            \"stream\": False\n        }\n\n        if system_prompt:\n            request_data[\"system\"] = system_prompt\n\n        # Add supported parameters\n        supported_params = self.config_loader.get_supported_parameters(\n            self.provider_name, model\n        )\n\n        for param in supported_params:\n            param_value = self._get_parameter(param, kwargs, model)\n            if param_value is not None:\n                converted_value = self._convert_parameter_type(param, param_value, model)\n                request_data[param] = converted_value\n\n        # Make request\n        response = self._make_request(**request_data)\n        return self._parse_response(response, start_time, model)\n\n    def _make_request(self, **kwargs) -&gt; Any:\n        \"\"\"Make the actual API request to Ollama.\"\"\"\n        response = requests.post(\n            f\"{self.base_url}/api/generate\",\n            json=kwargs,\n            timeout=120\n        )\n        response.raise_for_status()\n        return response.json()\n\n    def _parse_response(self, response: Any, start_time: float, model: str) -&gt; LLMResponse:\n        \"\"\"Parse Ollama response into LLMResponse.\"\"\"\n        latency_ms = (time.time() - start_time) * 1000\n\n        return LLMResponse(\n            text=response.get(\"response\", \"\"),\n            provider=\"ollama\",\n            model=model,\n            cost=0.0,  # Self-hosted = free\n            latency_ms=latency_ms,\n            metadata=response\n        )\n\n    def _calculate_cost(self, input_tokens: int, output_tokens: int, model: str) -&gt; float:\n        \"\"\"Calculate cost (free for self-hosted).\"\"\"\n        return 0.0\n</code></pre>"},{"location":"getting-started/#2-register-the-provider","title":"2. Register the Provider","text":"<p>Use the <code>register_provider()</code> function for clean registration:</p> <pre><code>from octuner import register_provider\nfrom my_custom_provider import OllamaProvider\n\n# Register the custom provider - clean and simple!\nregister_provider('ollama', OllamaProvider)\n</code></pre> <p>You can also list available providers:</p> <pre><code>from octuner import list_providers\n\nprint(list_providers())  # ['openai', 'gemini', 'ollama']\n</code></pre>"},{"location":"getting-started/#3-create-a-config-file","title":"3. Create a Config File","text":"<p>Create <code>ollama_config.yaml</code>:</p> <pre><code>providers:\n  ollama:\n    default_model: \"llama2\"\n\n    available_models:\n      - llama2\n      - mistral\n      - codellama\n\n    # Free for self-hosted\n    pricing_usd_per_1m_tokens:\n      llama2: [0.0, 0.0]\n      mistral: [0.0, 0.0]\n      codellama: [0.0, 0.0]\n\n    model_capabilities:\n      llama2:\n        supported_parameters: [temperature, top_p, top_k]\n        parameters:\n          temperature:\n            type: float\n            range: [0.0, 1.0]\n            default: 0.7\n          top_p:\n            type: float\n            range: [0.0, 1.0]\n            default: 0.9\n          top_k:\n            type: int\n            range: [1, 100]\n            default: 40\n        forced_parameters: {}\n\n      mistral:\n        supported_parameters: [temperature, top_p, top_k]\n        parameters:\n          temperature:\n            type: float\n            range: [0.0, 1.0]\n            default: 0.7\n          top_p:\n            type: float\n            range: [0.0, 1.0]\n            default: 0.9\n          top_k:\n            type: int\n            range: [1, 100]\n            default: 40\n        forced_parameters: {}\n\n      codellama:\n        supported_parameters: [temperature, top_p, top_k]\n        parameters:\n          temperature:\n            type: float\n            range: [0.0, 1.0]\n            default: 0.7\n          top_p:\n            type: float\n            range: [0.0, 1.0]\n            default: 0.9\n          top_k:\n            type: int\n            range: [1, 100]\n            default: 40\n        forced_parameters: {}\n</code></pre>"},{"location":"getting-started/#4-use-your-custom-provider","title":"4. Use Your Custom Provider","text":"<pre><code>from octuner import MultiProviderTunableLLM, AutoTuner, register_provider\nfrom my_custom_provider import OllamaProvider\n\n# Register custom provider with clean API\nregister_provider('ollama', OllamaProvider)\n\n# Create tunable LLM with your custom provider\nllm = MultiProviderTunableLLM(\n    config_file=\"ollama_config.yaml\",\n    default_provider=\"ollama\",\n    provider_configs={\n        \"ollama\": {\"base_url\": \"http://localhost:11434\"}\n    }\n)\n\n# Use it\nresponse = llm.call(\"What is machine learning?\")\nprint(response.text)\n\n# Optimize it\ndataset = [\n    {\"input\": \"Explain AI\", \"target\": \"artificial intelligence\"},\n    {\"input\": \"What is ML?\", \"target\": \"machine learning\"},\n]\n\ntuner = AutoTuner(\n    component=llm,\n    entrypoint=lambda llm, inp: llm.call(inp).text,\n    dataset=dataset,\n    metric=lambda output, target: 1.0 if target.lower() in output.lower() else 0.0\n)\n\nresult = tuner.search(n_trials=10)\nprint(f\"Best config: {result.best_trial.params}\")\n</code></pre>"},{"location":"getting-started/#key-implementation-requirements","title":"Key Implementation Requirements","text":"<p>When creating a custom provider, you must implement:</p> <ol> <li><code>__init__(self, config_loader, **kwargs)</code> - Initialize with config loader</li> <li><code>provider_name</code> - Set a unique string identifier</li> <li><code>call(prompt, system_prompt, **kwargs)</code> - Main entry point for LLM calls</li> <li><code>_make_request(**kwargs)</code> - Handle the actual API request</li> <li><code>_parse_response(response)</code> - Parse API response to <code>LLMResponse</code></li> <li><code>_calculate_cost(input_tokens, output_tokens, model)</code> - Calculate costs</li> </ol> <p>The base class provides: - <code>_get_parameter()</code> - Get parameters from config or kwargs - <code>_convert_parameter_type()</code> - Automatic type conversion - <code>get_cost_per_token()</code> - Get pricing from config - <code>get_available_models()</code> - Get models from config</p>"},{"location":"getting-started/#provider-registration-api","title":"Provider Registration API","text":"<p>Octuner provides clean functions for managing providers:</p> <pre><code>from octuner import (\n    register_provider,      # Add a custom provider\n    unregister_provider,    # Remove a provider\n    list_providers          # List all registered providers\n)\n\n# Register\nregister_provider('ollama', OllamaProvider)\n\n# Check what's available\nprint(list_providers())  # ['openai', 'gemini', 'ollama']\n\n# Unregister if needed\nunregister_provider('ollama')\n</code></pre>"},{"location":"getting-started/#multi-provider-optimization","title":"Multi-Provider Optimization","text":"<p>You can even optimize across cloud and self-hosted providers:</p> <pre><code># multi_config.yaml\nproviders:\n  openai:\n    # ... OpenAI config ...\n\n  ollama:\n    # ... Ollama config ...\n</code></pre> <pre><code>from octuner import MultiProviderTunableLLM, register_provider\n\n# Register custom provider\nregister_provider('ollama', OllamaProvider)\n\n# Octuner will now optimize across both!\nllm = MultiProviderTunableLLM(config_file=\"multi_config.yaml\")\nresult = tuner.search(n_trials=50)  # Test both OpenAI and Ollama\n</code></pre> <p>This allows you to discover whether a self-hosted model can match cloud performance for your specific use case!</p>"},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete API documentation and detailed examples</li> <li>Contributing - Learn how to contribute to Octuner</li> <li>Installation - Review installation and setup options</li> </ul> <p>Key Principle: Octuner is library-first. You explicitly provide config files, no global defaults or hidden state.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>pip or uv</li> </ul>"},{"location":"installation/#1-install","title":"1. Install","text":"<pre><code># pip\npip install octuner\n\n# or uv (recommended)\nuv add octuner\n</code></pre>"},{"location":"installation/#2-configure-provider-credentials","title":"2. Configure provider credentials","text":"<p>Set the API keys for the providers you plan to use:</p> <pre><code>export OPENAI_API_KEY=\"sk-your-openai-key\"\nexport GOOGLE_API_KEY=\"your-google-api-key\"\n</code></pre>"},{"location":"installation/#3-create-an-explicit-yaml-config","title":"3. Create an explicit YAML config","text":"<p>Octuner is library-first and uses explicit configs only (no global defaults). Copy a starter template and keep it in your repo:</p> <pre><code>mkdir -p configs\ncp config_templates/multi_provider.yaml configs/llm.yaml   # or openai_basic.yaml / gemini_basic.yaml\n</code></pre> <p>Edit <code>configs/llm.yaml</code> to include the providers/models you want enabled.</p>"},{"location":"installation/#4-use-in-your-application","title":"4. Use in your application","text":"<p>See Adapt existing code in Getting Started for concise before/after examples and a service wrapper pattern.</p> <ul> <li>Getting Started \u2192 Adapt existing code</li> <li>Then return to Optimization to tune parameters</li> </ul>"},{"location":"installation/#verify","title":"Verify","text":"<pre><code>import octuner\nprint(\"Octuner version:\", octuner.__version__)\n\nfrom octuner import MultiProviderTunableLLM\n_ = MultiProviderTunableLLM(config_file=\"configs/llm.yaml\")\nprint(\"Config loaded OK\")\n</code></pre>"},{"location":"installation/#next-steps","title":"Next steps","text":"<ul> <li>Read the short Getting Started guide</li> <li>Explore the API Reference for detailed documentation</li> <li>Learn how to contribute to Octuner</li> </ul> <p>Note: Always pass your YAML <code>config_file</code> explicitly when constructing <code>MultiProviderTunableLLM</code>.</p>"},{"location":"reference/","title":"API Reference","text":"<p>This section contains the auto-generated documentation for Octuner's classes, methods, and functions extracted from the source code docstrings.</p>"},{"location":"reference/#core-components","title":"Core Components","text":""},{"location":"reference/#autotuner","title":"AutoTuner","text":"<p>The main entry point for auto-tuning LLM components. Orchestrates discovery, execution, and optimization.</p>"},{"location":"reference/#octuner.optimization.auto.AutoTuner","title":"<code>octuner.optimization.auto.AutoTuner(component, entrypoint=None, dataset=None, metric=None, entrypoint_function=None, metric_function=None, max_workers=1, optimization_mode='pareto', n_trials=120, constraints=None, scalarization_weights=None)</code>","text":"<p>This is the main orchestrator for auto-tuning the LLM components. As the central class of the Octuner optimization system, it that coordinates the entire parameter optimization workflow. It automatically discovers tunable parameters in complex component hierarchies, builds search spaces, and runs optimization algorithms to find the best configuration.</p> <p>How it works:</p> <ol> <li> <p>Component Discovery: Automatically finds all tunable components in the    object hierarchy using ComponentDiscovery, identifying parameters that can    be optimized (temperature, model selection, provider choices, etc.)</p> </li> <li> <p>Search Space Construction: Builds a multi-dimensional search space from    discovered parameters, defining the optimization landscape with parameter    types, ranges, and constraints.</p> </li> <li> <p>Optimization Execution: Runs intelligent search algorithms (Pareto,    constrained, or scalarized optimization) to explore the search space and    find optimal parameter combinations.</p> </li> <li> <p>Result Analysis: Provides comprehensive results including the best parameters,    performance metrics, and detailed trial information for analysis and application.</p> </li> </ol> <p>Key Features:</p> <ul> <li>Multi-Objective Optimization: Supports Pareto optimization for balancing   quality, cost, and latency objectives</li> <li>Constraint Handling: Supports hard constraints for real-world deployment   requirements</li> <li>Scalarization: Converts multi-objective problems to single-objective   optimization with custom weights</li> <li>Parallel Execution: Supports concurrent trial execution for faster optimization</li> <li>Flexible Filtering: Include/exclude specific parameters to focus optimization</li> <li>Reproducible Results: Seed support for consistent optimization runs</li> </ul> Example <pre><code>from octuner import AutoTuner, MultiProviderTunableLLM\n\n# Create a tunable component\nllm = MultiProviderTunableLLM(config_file=\"config.yaml\")\n\n# Define evaluation function and dataset\ndef evaluate(component, input_data):\n    result = component.call(input_data[\"text\"])\n    return {\"quality\": compute_quality(result, input_data[\"target\"])}\n\ndataset = [{\"text\": \"Hello\", \"target\": \"Hi there\"}]\n\n# Create and configure tuner\ntuner = AutoTuner(\n    component=llm,\n    entrypoint=evaluate,\n    dataset=dataset,\n    metric=lambda output, target: output[\"quality\"]\n)\n\n# Focus on specific parameters\ntuner.include([\"*.temperature\", \"*.provider_model\"])\n\n# Run optimization\nresult = tuner.search(max_trials=50, mode=\"pareto\")\n\n# Apply the best parameters\nfrom octuner import apply_best\napply_best(llm, result.best_parameters)\n</code></pre> <p>Optimization Modes:</p> <pre><code>- **\"pareto\"**: Multi-objective optimization finding Pareto-optimal solutions\n- **\"constrained\"**: Single-objective optimization with hard constraints\n- **\"scalarized\"**: Multi-objective converted to single-objective with weights\n</code></pre> <p>Initialize the AutoTuner with a component and evaluation setup.</p> <p>This constructor sets up the AutoTuner with all necessary components for parameter optimization. It validates the inputs and prepares the internal state for discovery and optimization.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>Any</code> <p>The component to optimize. Must contain tunable parameters       (implement TunableMixin or be registered as tunable). Can be       a single component or a complex hierarchy of components.</p> required <code>entrypoint</code> <code>EntrypointFunction</code> <p>Function that evaluates the component with input data.        Called as <code>entrypoint(component, input_data)</code> for each        dataset item. Should return a dictionary with metrics.        (Legacy parameter name)</p> <code>None</code> <code>dataset</code> <code>Dataset</code> <p>List of input/target pairs for evaluation. Each item should     contain the input data and expected output for evaluation.</p> <code>None</code> <code>metric</code> <code>MetricFunction</code> <p>Function that computes quality scores from evaluation results.    Called as <code>metric(output, target)</code> where output is the result    from entrypoint and target is the expected output.    (Legacy parameter name)</p> <code>None</code> <code>entrypoint_function</code> <code>EntrypointFunction</code> <p>Same as entrypoint but with clearer naming.</p> <code>None</code> <code>metric_function</code> <code>MetricFunction</code> <p>Same as metric but with clearer naming.</p> <code>None</code> <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent workers for parallel         evaluation during optimization trials. Higher values         speed up optimization but use more resources.</p> <code>1</code> <code>optimization_mode</code> <code>str</code> <p>Optimization strategy to use. Options:              - \"pareto\": Multi-objective optimization (default)              - \"constrained\": Single-objective with constraints              - \"scalarized\": Multi-objective with custom weights</p> <code>'pareto'</code> <code>n_trials</code> <code>int</code> <p>Default number of optimization trials to run. Can be      overridden in search() calls.</p> <code>120</code> <code>constraints</code> <code>Optional[Constraints]</code> <p>Hard constraints for constrained optimization mode.         Dictionary with constraint names and values.</p> <code>None</code> <code>scalarization_weights</code> <code>Optional[ScalarizationWeights]</code> <p>Weights for scalarized optimization mode.                  Dictionary mapping objective names to weights.</p> <code>None</code>"},{"location":"reference/#octuner.optimization.auto.AutoTuner.from_component","title":"<code>from_component(*, component, entrypoint, dataset, metric, max_workers=1)</code>  <code>classmethod</code>","text":"<p>Create an AutoTuner instance using the factory pattern. It's the recommended way to create AutoTuner instances for most use cases.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>Any</code> <p>The component to optimize. Must contain tunable parameters       (implement TunableMixin or be registered as tunable).</p> required <code>entrypoint</code> <code>EntrypointFunction</code> <p>Function that evaluates the component with input data.        Called as <code>entrypoint(component, input_data)</code> for each        dataset item. Should return a dictionary with metrics.</p> required <code>dataset</code> <code>Dataset</code> <p>List of input/target pairs for evaluation. Each item should     contain the input data and expected output for evaluation.</p> required <code>metric</code> <code>MetricFunction</code> <p>Function that computes quality scores from evaluation results.    Called as <code>metric(output, target)</code> where output is the result    from entrypoint and target is the expected output.</p> required <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent workers for parallel         evaluation during optimization trials. Default is 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>AutoTuner</code> <p>Configured AutoTuner instance ready for optimization.</p> Example <pre><code># Create tuner using factory method\ntuner = AutoTuner.from_component(\n    component=my_llm,\n    entrypoint=lambda comp, data: comp.call(data[\"text\"]),\n    dataset=test_dataset,\n    metric=lambda output, target: compute_quality(output, target),\n    max_workers=4\n)\n\n# Run optimization\nresult = tuner.search(max_trials=100)\n</code></pre>"},{"location":"reference/#octuner.optimization.auto.AutoTuner.include","title":"<code>include(patterns)</code>","text":"<p>This method allows to narrow down the optimization to only specific parameters or components, reducing the search space and focusing on the most important parameters for your use case.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>List[str]</code> <p>List of glob patterns to include in optimization.      Only parameters matching at least one pattern will be      included in the search space. Examples:      - [\".temperature\"]: Include all temperature parameters      - [\".provider_model\"]: Include all provider/model selections      - [\"classifier_llm.\"]: Include all parameters in classifier_llm      - [\".max_tokens\", \"*.top_p\"]: Include max_tokens and top_p</p> required <p>Returns:</p> Type Description <code>AutoTuner</code> <p>Self for method chaining, allowing fluent interface.</p> Example <pre><code># Focus on core LLM parameters\ntuner.include([\"*.temperature\", \"*.max_tokens\", \"*.provider_model\"])\n\n# Focus on specific components\ntuner.include([\"classifier_llm.*\", \"confidence_llm.*\"])\n\n# Chain with exclude for fine control\ntuner.include([\"*.temperature\"]).exclude([\"*.verbose\"])\n</code></pre> Note <p>Include patterns are applied before exclude patterns. If no include patterns are set, all discovered parameters are considered for inclusion.</p>"},{"location":"reference/#octuner.optimization.auto.AutoTuner.exclude","title":"<code>exclude(patterns)</code>","text":"<p>This method allows to exclude certain parameters from the optimization process, typically to remove debug parameters, verbose settings, or other parameters that shouldn't be optimized.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>List[str]</code> <p>List of glob patterns to exclude from optimization.      Parameters matching any pattern will be removed from      the search space. Examples:      - [\".verbose\"]: Exclude all verbose parameters      - [\".debug\", \".log_level\"]: Exclude debug and logging parameters      - [\".frequency_penalty\"]: Exclude specific parameters      - [\"nested.component.*\"]: Exclude all parameters in nested.component</p> required <p>Returns:</p> Type Description <code>AutoTuner</code> <p>Self for method chaining</p> Example <pre><code># Exclude debug parameters\ntuner.exclude([\"*.verbose\", \"*.debug\", \"*.log_level\"])\n\n# Exclude less important parameters\ntuner.exclude([\"*.frequency_penalty\", \"*.presence_penalty\"])\n\n# Chain with include for precise control\ntuner.include([\"*.temperature\"]).exclude([\"*.verbose\"])\n</code></pre> Note <p>Exclude patterns are applied after include patterns. If a parameter matches both include and exclude patterns, it will be excluded.</p>"},{"location":"reference/#octuner.optimization.auto.AutoTuner.build_search_space","title":"<code>build_search_space()</code>","text":"<p>Performs the component discovery process and constructs the search space that defines the optimization landscape. It's automatically called by search() if not already built, but can be called manually to inspect the discovered parameters.</p> <p>The discovery process:</p> <ol> <li>Component Traversal: Recursively explores the component hierarchy</li> <li>Parameter Detection: Identifies all tunable parameters using    TunableMixin protocol or registry-based detection</li> <li>Search Space Construction: Builds a flat dictionary mapping    parameter paths to their definitions and constraints</li> <li>Validation: Ensures at least one tunable parameter is found</li> </ol> <p>The resulting search space contains:</p> <ul> <li>Parameter paths (e.g., \"llm.temperature\", \"classifier.max_tokens\")</li> <li>Parameter types (\"float\", \"int\", \"choice\", \"bool\")</li> <li>Value ranges or choices for each parameter</li> <li>Default values where applicable</li> </ul> <p>Returns:</p> Type Description <code>None</code> <p>None. The search space is stored in self.search_space.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no tunable components are found in the component        hierarchy. This usually means the component doesn't        implement TunableMixin or isn't registered as tunable.</p> Example <pre><code># Build search space manually\ntuner.build_search_space()\n\n# Inspect discovered parameters\nsummary = tuner.get_search_space_summary()\nprint(f\"Found {summary['total_parameters']} tunable parameters\")\nprint(f\"Parameter types: {summary['parameter_types']}\")\n\n# View specific parameters\nfor param_path, param_def in tuner.search_space.items():\n    print(f\"{param_path}: {param_def}\")\n</code></pre> Note <p>This method is idempotent - calling it multiple times has no effect after the first successful call. The search space is cached until the component structure changes.</p>"},{"location":"reference/#octuner.optimization.auto.AutoTuner.search","title":"<code>search(*, max_trials=120, mode='pareto', constraints=None, scalarization_weights=None, replicates=1, timeout=None, seed=None)</code>","text":"<p>Run the optimization search to find the best parameter configuration.</p> <p>This is the main method that orchestrates the entire optimization process. It automatically discovers tunable parameters, sets up the optimization environment, and runs intelligent search algorithms to find optimal parameter combinations.</p> <p>The optimization process:</p> <ol> <li>Discovery: Finds all tunable parameters in the component hierarchy</li> <li>Search Space Setup: Builds the multi-dimensional search space</li> <li>Optimization: Runs the specified optimization algorithm</li> <li>Result Analysis: Analyzes results and returns comprehensive findings</li> </ol> <p>Parameters:</p> Name Type Description Default <code>max_trials</code> <code>int</code> <p>Maximum number of optimization trials to run. More trials        generally lead to better results but take longer. Typical        values range from 50-500 depending on search space size.</p> <code>120</code> <code>mode</code> <code>OptimizationMode</code> <p>Optimization strategy to use:  - \"pareto\": Multi-objective optimization finding Pareto-optimal    solutions that balance multiple objectives (quality, cost, latency)  - \"constrained\": Single-objective optimization with hard constraints    for real-world deployment requirements  - \"scalarized\": Multi-objective converted to single-objective    using custom weights for different objectives</p> <code>'pareto'</code> <code>constraints</code> <code>Optional[Constraints]</code> <p>Hard constraints for constrained optimization mode.         Dictionary with constraint names and maximum values.         Example: {\"max_cost\": 0.01, \"max_latency_ms\": 1000}</p> <code>None</code> <code>scalarization_weights</code> <code>Optional[ScalarizationWeights]</code> <p>Weights for scalarized optimization mode.                  Dictionary mapping objective names to weights.                  Weights should sum to 1.0 for best results.                  Example: {\"quality\": 0.7, \"cost\": 0.3}</p> <code>None</code> <code>replicates</code> <code>int</code> <p>Number of replicates per trial for statistical robustness.        Higher values reduce noise but increase computation time.        Default is 1 for faster optimization.</p> <code>1</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time in seconds for the entire optimization process.     If None, optimization runs until max_trials is reached.</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducible optimization results. Use the same  seed to get identical results across runs.</p> <code>None</code> <p>Returns:</p> Type Description <code>SearchResult</code> <p>SearchResult containing:</p> <code>SearchResult</code> <ul> <li>best_trial: The best performing trial with optimal parameters</li> </ul> <code>SearchResult</code> <ul> <li>all_trials: List of all trials for detailed analysis</li> </ul> <code>SearchResult</code> <ul> <li>best_parameters: Dictionary of best parameter values</li> </ul> <code>SearchResult</code> <ul> <li>optimization_mode: The mode used for optimization</li> </ul> <code>SearchResult</code> <ul> <li>metrics_summary: Statistical summary of all trials</li> </ul> Example <pre><code># Basic optimization\nresult = tuner.search()\n\n# Advanced optimization with constraints\nresult = tuner.search(\n    max_trials=200,\n    mode=\"constrained\",\n    constraints={\"max_cost\": 0.01, \"max_latency_ms\": 500},\n    replicates=3,\n    timeout=3600,  # 1 hour timeout\n    seed=42\n)\n\n# Multi-objective optimization with custom weights\nresult = tuner.search(\n    mode=\"scalarized\",\n    scalarization_weights={\"quality\": 0.8, \"cost\": 0.2},\n    max_trials=100\n)\n\n# Access results\nprint(f\"Best quality: {result.best_trial.metrics.quality}\")\nprint(f\"Best parameters: {result.best_parameters}\")\n</code></pre> Note <p>The first call to search() will automatically discover tunable components and build the search space. Subsequent calls reuse the existing search space unless the component structure changes.</p>"},{"location":"reference/#octuner.optimization.auto.AutoTuner.get_search_space_summary","title":"<code>get_search_space_summary()</code>","text":"<p>Provides detailed information about the tunable parameters discovered in the component hierarchy, including counts, types, and component distribution. Useful for understanding the optimization landscape before running optimization.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing search space summary with keys: \"total_parameters\", \"parameter_types\", \"components\"</p> Example <pre><code># Build search space and get summary\ntuner.build_search_space()\nsummary = tuner.get_search_space_summary()\n\nprint(f\"Total parameters: {summary['total_parameters']}\")\nprint(f\"Parameter types: {summary['parameter_types']}\")\nprint(f\"Components: {summary['components']}\")\n\n# Output might be:\n# Total parameters: 12\n# Parameter types: {'float': 6, 'choice': 4, 'int': 2}\n# Components: {'llm': 8, 'classifier_llm': 4}\n</code></pre> Note <p>This method requires the search space to be built first. It's automatically called by search() if not already built.</p>"},{"location":"reference/#octuner.optimization.auto.AutoTuner.get_current_parameters","title":"<code>get_current_parameters()</code>","text":"<p>Get the current parameter values on the component.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary of current parameter values</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If search space has not been built yet</p>"},{"location":"reference/#tunable-llm","title":"Tunable LLM","text":""},{"location":"reference/#multiprovidertunablellm","title":"MultiProviderTunableLLM","text":"<p>A tunable LLM wrapper that optimizes provider, model, and parameter selection across multiple LLM providers.</p>"},{"location":"reference/#octuner.tunable.tunable_llm.MultiProviderTunableLLM","title":"<code>octuner.tunable.tunable_llm.MultiProviderTunableLLM(config_file, default_provider='openai', default_model=None, provider_configs=None)</code>","text":"<p>               Bases: <code>TunableMixin</code></p> <p>A tunable LLM wrapper that optimizes provider, model, and parameter selection.</p> <p>This class allows the optimization system to discover the best combination of: - LLM provider (OpenAI, Gemini, etc.) - Model within that provider - Model-specific parameters (temperature, max_tokens, etc.)</p> <p>Configuration is defined explicitly via YAML files.</p> Example <p>llm = MultiProviderTunableLLM(config_file=\"my_llm_config.yaml\") response = llm.call(\"What is the capital of France?\") print(response.text)</p> <p>Initialize the tunable LLM with explicit configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to YAML configuration file (required)</p> required <code>default_provider</code> <code>str</code> <p>Default provider to use ('openai', 'gemini') </p> <code>'openai'</code> <code>default_model</code> <code>Optional[str]</code> <p>Default model to use (if None, uses provider's default)</p> <code>None</code> <code>provider_configs</code> <code>Optional[Dict[str, Dict[str, Any]]]</code> <p>Configuration for each provider (API keys, etc.)</p> <code>None</code>"},{"location":"reference/#octuner.tunable.tunable_llm.MultiProviderTunableLLM.call","title":"<code>call(prompt, system_prompt=None, **kwargs)</code>","text":"<p>Make an LLM call using the current provider and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The user prompt</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters that override instance settings</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMResponse</code> <p>LLMResponse with the result</p>"},{"location":"reference/#octuner.tunable.tunable_llm.MultiProviderTunableLLM.llm_eq_cost","title":"<code>llm_eq_cost(*, input_tokens=None, output_tokens=None, metadata=None)</code>","text":"<p>Calculate the cost of an LLM call based on current provider and model.</p> <p>Parameters:</p> Name Type Description Default <code>input_tokens</code> <p>Number of input tokens</p> <code>None</code> <code>output_tokens</code> <p>Number of output tokens</p> <code>None</code> <code>metadata</code> <p>Additional metadata from the LLM call</p> <code>None</code> <p>Returns:</p> Type Description <p>Cost in USD, or None if tokens are not available</p>"},{"location":"reference/#octuner.tunable.tunable_llm.MultiProviderTunableLLM.get_current_provider_info","title":"<code>get_current_provider_info()</code>","text":"<p>Get information about the current provider and model.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with provider info</p>"},{"location":"reference/#octuner.tunable.tunable_llm.MultiProviderTunableLLM.set_provider_configs","title":"<code>set_provider_configs(configs)</code>","text":"<p>Update provider configurations (API keys, base URLs, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>configs</code> <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary mapping provider names to configuration dicts</p> required"},{"location":"reference/#tunablemixin","title":"TunableMixin","text":"<p>Base mixin class that makes any component tunable by the optimization system.</p>"},{"location":"reference/#octuner.tunable.mixin.TunableMixin","title":"<code>octuner.tunable.mixin.TunableMixin()</code>","text":"<p>Mixin class for LLM components that can be auto-tuned.</p> <p>Components can be made tunable by either: 1. Using instance methods like mark_as_tunable() (legacy approach) 2. Programmatic registration using register_tunable_class() (recommended)</p> Example <p>class MyLLM(TunableMixin):     def init(self):         super().init()         # Legacy approach         self.mark_as_tunable(\"temperature\", \"float\", (0.0, 1.0), 0.7)</p> <pre><code>    # Or programmatic registration (recommended)\n    from octuner.tunable.registry import register_tunable_class\n    register_tunable_class(\n        self.__class__,\n        params={\n            \"temperature\": (\"float\", 0.0, 1.0),\n            \"max_tokens\": (\"int\", 64, 4096),\n        },\n        call_method=\"send_prompt\"\n    )\n</code></pre> <p>Initialize the tunable mixin.</p>"},{"location":"reference/#octuner.tunable.mixin.TunableMixin.mark_as_tunable","title":"<code>mark_as_tunable(param_name, param_type, range_vals, default=None)</code>","text":"<p>Mark a parameter as tunable.</p> <p>Parameters:</p> Name Type Description Default <code>param_name</code> <code>str</code> <p>Name of the parameter</p> required <code>param_type</code> <code>str</code> <p>Type of parameter (\"float\", \"int\", \"choice\", \"bool\")</p> required <code>range_vals</code> <code>Tuple[Any, Any]</code> <p>Range tuple (min, max) for numeric types, choices for choice type</p> required <code>default</code> <code>Any</code> <p>Default value for the parameter</p> <code>None</code>"},{"location":"reference/#octuner.tunable.mixin.TunableMixin.get_tunable_parameters","title":"<code>get_tunable_parameters()</code>","text":"<p>Get all tunable parameters.</p> <p>Returns:</p> Type Description <code>Dict[str, Dict[str, Any]]</code> <p>Dictionary of tunable parameter definitions</p>"},{"location":"reference/#octuner.tunable.mixin.TunableMixin.is_tunable","title":"<code>is_tunable(param_name)</code>","text":"<p>Check if a parameter is tunable.</p> <p>Parameters:</p> Name Type Description Default <code>param_name</code> <code>str</code> <p>Name of the parameter</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the parameter is tunable</p>"},{"location":"reference/#octuner.tunable.mixin.TunableMixin.get_param_info","title":"<code>get_param_info(param_name)</code>","text":"<p>Get information about a tunable parameter.</p> <p>Parameters:</p> Name Type Description Default <code>param_name</code> <code>str</code> <p>Name of the parameter</p> required <p>Returns:</p> Type Description <code>Optional[Dict[str, Any]]</code> <p>Parameter info dictionary or None if not found</p>"},{"location":"reference/#octuner.tunable.mixin.TunableMixin.llm_eq_cost","title":"<code>llm_eq_cost(*, input_tokens=None, output_tokens=None, metadata=None)</code>","text":"<p>Calculate the cost of an LLM call (optional).</p> <p>Override this method to enable cost tracking during optimization.</p> <p>Parameters:</p> Name Type Description Default <code>input_tokens</code> <code>Optional[int]</code> <p>Number of input tokens</p> <code>None</code> <code>output_tokens</code> <code>Optional[int]</code> <p>Number of output tokens  </p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata from the LLM call</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[float]</code> <p>Cost in your preferred currency, or None to disable cost tracking</p>"},{"location":"reference/#providers","title":"Providers","text":""},{"location":"reference/#base-provider","title":"Base Provider","text":"<p>Abstract base class for LLM providers and standard response format.</p>"},{"location":"reference/#octuner.providers.base.BaseLLMProvider","title":"<code>octuner.providers.base.BaseLLMProvider(config_loader, **kwargs)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for implementing custom LLM providers in Octuner.</p> <p>This class serves as the foundation for creating custom LLM provider implementations, enabling to integrate own self-hosted models, proprietary APIs, or any other LLM service.</p> <p>Key Features: - Configuration-driven: Integrates with YAML-based configuration system - Parameter optimization: Supports automatic parameter tuning through the config loader - Type conversion: Automatic parameter type conversion based on configuration - Cost tracking: Built-in cost calculation and token usage tracking - Standardized responses: Returns consistent LLMResponse objects across all providers</p> <p>To create a custom provider, you must:</p> <ol> <li>Inherit from BaseLLMProvider and set the <code>provider_name</code> attribute</li> <li>Implement abstract methods:</li> <li><code>call()</code>: Main interface for making LLM requests</li> <li><code>_make_request()</code>: Low-level API communication</li> <li><code>_parse_response()</code>: Convert raw API response to LLMResponse</li> <li> <p><code>_calculate_cost()</code>: Calculate cost based on token usage</p> </li> <li> <p>Create a configuration file (YAML) defining:</p> </li> <li>Available models and their parameters</li> <li>Parameter types, ranges, and defaults</li> <li>Pricing information for cost calculation</li> <li>Provider-specific settings</li> </ol> <p>Example Usage: <pre><code>from octuner.providers.base import BaseLLMProvider, LLMResponse\nfrom octuner.config.loader import ConfigLoader\n\nclass CustomProvider(BaseLLMProvider):\n    def __init__(self, config_loader, **kwargs):\n        super().__init__(config_loader, **kwargs)\n        self.provider_name = \"custom\"\n        # Initialize your API client here\n\n    def call(self, prompt: str, system_prompt: Optional[str] = None, **kwargs) -&gt; LLMResponse:\n        # Implementation details...\n        pass\n\n    # Implement other abstract methods...\n\n# Usage with configuration\nconfig_loader = ConfigLoader(\"my_custom_config.yaml\")\nprovider = CustomProvider(config_loader, api_key=\"your-key\")\nresponse = provider.call(\"Hello, world!\")\n</code></pre></p> <p>Configuration File Structure: <pre><code>providers:\n  custom:\n    default_model: \"my-model-v1\"\n    available_models: [\"my-model-v1\", \"my-model-v2\"]\n    pricing_usd_per_1m_tokens:\n      my-model-v1: [0.5, 1.0]  # [input_cost, output_cost]\n    model_capabilities:\n      my-model-v1:\n        supported_parameters: [\"temperature\", \"max_tokens\"]\n        parameters:\n          temperature:\n            type: float\n            range: [0.0, 2.0]\n            default: 0.7\n          max_tokens:\n            type: int\n            range: [1, 4000]\n            default: 1000\n</code></pre></p> <p>This constructor sets up the provider with access to the configuration system and stores any provider-specific parameters. The config_loader is mandatory as it provides access to model capabilities, parameter definitions, and pricing.</p> <p>Parameters:</p> Name Type Description Default <code>config_loader</code> <code>ConfigLoader</code> <p>Configuration loader instance that provides access to YAML configuration files. This is mandatory.</p> required <code>**kwargs</code> <p>Provider-specific configuration parameters. These can override default values from the configuration file.</p> <code>{}</code>"},{"location":"reference/#octuner.providers.base.BaseLLMProvider.call","title":"<code>call(prompt, system_prompt=None, **kwargs)</code>  <code>abstractmethod</code>","text":"<p>This is the main interface method that users will call to interact with the LLM provider. It handles parameter resolution, type conversion, API communication, and response parsing.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The user prompt/query to send to the LLM</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt that sets the context or behavior for the LLM. If None, no system prompt is used.</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters that can override configuration defaults.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>LLMResponse</code> <code>LLMResponse</code> <p>Standardized response object containing:</p> Example Implementation <pre><code>def call(self, prompt: str, system_prompt: Optional[str] = None, **kwargs) -&gt; LLMResponse:\n    import time\n    start_time = time.time()\n\n    # Get model and parameters\n    model = self._get_parameter(\"model\", kwargs, \"default-model\")\n    temperature = self._get_parameter(\"temperature\", kwargs, model)\n    max_tokens = self._get_parameter(\"max_tokens\", kwargs, model)\n\n    # Convert types\n    temperature = self._convert_parameter_type(\"temperature\", temperature, model)\n    max_tokens = self._convert_parameter_type(\"max_tokens\", max_tokens, model)\n\n    # Prepare API request\n    api_params = {\n        \"model\": model,\n        \"prompt\": prompt,\n        \"system_prompt\": system_prompt,\n        \"temperature\": temperature,\n        \"max_tokens\": max_tokens\n    }\n\n    # Make API call\n    response = self._make_request(**api_params)\n\n    # Parse and return response\n    result = self._parse_response(response)\n    result.latency_ms = (time.time() - start_time) * 1000\n    return result\n</code></pre>"},{"location":"reference/#octuner.providers.base.BaseLLMProvider.get_cost_per_token","title":"<code>get_cost_per_token(model)</code>","text":"<p>Get the cost per input and output token for a model.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier</p> required <p>Returns:</p> Type Description <code>Tuple[float, float]</code> <p>Tuple of (input_cost_per_1M_tokens, output_cost_per_1M_tokens)</p>"},{"location":"reference/#octuner.providers.base.BaseLLMProvider.get_available_models","title":"<code>get_available_models()</code>","text":"<p>Get list of available models for this provider.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of model identifiers</p>"},{"location":"reference/#octuner.providers.base.LLMResponse","title":"<code>octuner.providers.base.LLMResponse(text, provider=None, model=None, cost=None, input_tokens=None, output_tokens=None, latency_ms=None, metadata=None)</code>  <code>dataclass</code>","text":"<p>Standard response format from all LLM providers.</p>"},{"location":"reference/#openai-provider","title":"OpenAI Provider","text":"<p>OpenAI provider implementation using the OpenAI API.</p>"},{"location":"reference/#octuner.providers.openai.OpenAIProvider","title":"<code>octuner.providers.openai.OpenAIProvider(config_loader, **kwargs)</code>","text":"<p>               Bases: <code>BaseLLMProvider</code></p> <p>OpenAI provider implementation for Octuner.</p> <p>This module contains the OpenAI provider implementation using the Responses API.</p>"},{"location":"reference/#octuner.providers.openai.OpenAIProvider.call","title":"<code>call(prompt, system_prompt=None, use_websearch=False, **kwargs)</code>","text":"<p>Make a call to OpenAI Chat Completions API, with optional websearch support.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>The user prompt</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional system prompt</p> <code>None</code> <code>use_websearch</code> <code>bool</code> <p>Whether to enable websearch tool</p> <code>False</code> <code>**kwargs</code> <p>Additional parameters to override defaults (e.g., model, temperature, max_tokens, etc.)</p> <code>{}</code>"},{"location":"reference/#gemini-provider","title":"Gemini Provider","text":"<p>Google Gemini provider implementation using the google-generativeai SDK.</p>"},{"location":"reference/#octuner.providers.gemini.GeminiProvider","title":"<code>octuner.providers.gemini.GeminiProvider(config_loader, **kwargs)</code>","text":"<p>               Bases: <code>BaseLLMProvider</code></p> <p>Gemini provider implementation. Using the google-generativeai SDK.</p>"},{"location":"reference/#octuner.providers.gemini.GeminiProvider.call","title":"<code>call(prompt, system_prompt=None, use_websearch=False, **kwargs)</code>","text":"<p>Make a call to Gemini API, with optional websearch support.</p>"},{"location":"reference/#provider-registry","title":"Provider Registry","text":"<p>Functions for registering and retrieving LLM providers.</p>"},{"location":"reference/#octuner.providers.registry.register_provider","title":"<code>octuner.providers.registry.register_provider(name, provider_class)</code>","text":"<p>Register a custom LLM provider.</p> <p>This allows you to add custom providers for self-hosted LLMs or other services.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Provider name (e.g., 'ollama', 'vllm', 'custom')</p> required <code>provider_class</code> <code>Type[BaseLLMProvider]</code> <p>Provider class that inherits from BaseLLMProvider</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If provider_class doesn't inherit from BaseLLMProvider</p>"},{"location":"reference/#octuner.providers.registry.get_provider","title":"<code>octuner.providers.registry.get_provider(provider_name, config_loader, **kwargs)</code>","text":"<p>Get a provider instance by name.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider ('openai', 'gemini', or custom)</p> required <code>config_loader</code> <p>ConfigLoader for configuration-driven behavior (mandatory)</p> required <code>**kwargs</code> <p>Provider-specific configuration</p> <code>{}</code> <p>Returns:</p> Type Description <code>BaseLLMProvider</code> <p>Provider instance</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If provider is not supported</p>"},{"location":"reference/#octuner.providers.registry.list_providers","title":"<code>octuner.providers.registry.list_providers()</code>","text":"<p>Get list of all registered provider names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of provider names</p>"},{"location":"reference/#optimization","title":"Optimization","text":""},{"location":"reference/#llmoptimizer","title":"LLMOptimizer","text":"<p>Main optimizer class that uses Optuna to find optimal parameter configurations.</p>"},{"location":"reference/#octuner.optimization.optimizer.LLMOptimizer","title":"<code>octuner.optimization.optimizer.LLMOptimizer(search_space, mode='pareto', constraints=None, scalarization_weights=None, seed=None)</code>","text":"<p>Core optimization engine that uses Optuna to find optimal parameter configurations.</p> <p>LLMOptimizer is the main optimization engine that coordinates the search for optimal parameter values using Optuna's sophisticated optimization algorithms. It integrates with DatasetExecutor to evaluate parameter configurations and uses optimization strategies to determine the best solutions.</p> <p>The optimization process: 1. Parameter Suggestion: Uses Optuna to suggest parameter values from the search space 2. Trial Execution: Evaluates suggested parameters using DatasetExecutor 3. Objective Computation: Converts evaluation results to objective values using the strategy 4. Optimization: Uses Optuna's TPE sampler to intelligently explore the parameter space 5. Result Analysis: Extracts best parameters and trial results from completed optimization</p> <p>Key features: - Supports multiple optimization strategies (Pareto, constrained, scalarized) - Intelligent parameter suggestion using TPE (Tree-structured Parzen Estimator) - Robust error handling with fallback objective values for failed trials - Comprehensive trial result tracking and analysis - Integration with Optuna's pruning mechanism for constraint handling</p> <p>Initialize the LLMOptimizer with search space and optimization configuration.</p> <p>Parameters:</p> Name Type Description Default <code>search_space</code> <code>Dict[str, Tuple[ParamType, Any, Any]]</code> <p>Dictionary mapping parameter paths to their definitions.         Each definition is a tuple of (type, min_value, max_value)         for numeric parameters or (type, choices) for categorical parameters.</p> required <code>mode</code> <code>OptimizationMode</code> <p>Optimization strategy to use:  - \"pareto\": Multi-objective Pareto optimization (default)  - \"constrained\": Single-objective with hard constraints  - \"scalarized\": Multi-objective converted to single-objective</p> <code>'pareto'</code> <code>constraints</code> <code>Optional[Constraints]</code> <p>Hard constraints for constrained mode. Dictionary with         constraint names and maximum values (e.g., {\"cost_total\": 0.01}).</p> <code>None</code> <code>scalarization_weights</code> <code>Optional[ScalarizationWeights]</code> <p>Weights for scalarized mode. Defines relative                  importance of quality, cost, and latency objectives.</p> <code>None</code> <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducible optimization results. Use the same  seed to get identical optimization runs.</p> <code>None</code>"},{"location":"reference/#octuner.optimization.optimizer.LLMOptimizer.suggest_parameters","title":"<code>suggest_parameters(trial)</code>","text":"<p>Suggest parameter values for an Optuna trial.</p> <p>This method uses Optuna's intelligent parameter suggestion to generate parameter values from the search space. It handles different parameter types (float, int, choice, bool, list) and converts them to appropriate Optuna suggestion calls.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>Trial</code> <p>Optuna trial object for parameter suggestion</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary mapping parameter paths to suggested values</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If parameter ranges are invalid for numeric types</p> <code>ValueError</code> <p>If an unknown parameter type is encountered</p>"},{"location":"reference/#octuner.optimization.optimizer.LLMOptimizer.objective_function","title":"<code>objective_function(trial, executor, replicates=1)</code>","text":"<p>Objective function for Optuna optimization.</p> <p>This method serves as the objective function that Optuna optimizes. It suggests parameters, executes the trial using the DatasetExecutor, and converts the results to objective values using the optimization strategy.</p> <p>The function handles errors gracefully by returning fallback objective values for failed trials, ensuring the optimization process continues even when individual trials fail.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>Trial</code> <p>Optuna trial object for parameter suggestion</p> required <code>executor</code> <code>Any</code> <p>DatasetExecutor instance for trial evaluation</p> required <code>replicates</code> <code>int</code> <p>Number of replicates to run for statistical robustness</p> <code>1</code> <p>Returns:</p> Type Description <code>Tuple[float, ...]</code> <p>Tuple of objective values for Optuna optimization</p>"},{"location":"reference/#octuner.optimization.optimizer.LLMOptimizer.optimize","title":"<code>optimize(executor, max_trials=120, replicates=1, timeout=None)</code>","text":"<p>Run the optimization process to find optimal parameter configurations.</p> <p>This method orchestrates the entire optimization process using Optuna. It runs the specified number of trials, each evaluating a different parameter configuration, and returns comprehensive results from all trials.</p> <p>Parameters:</p> Name Type Description Default <code>executor</code> <code>Any</code> <p>DatasetExecutor instance for evaluating parameter configurations</p> required <code>max_trials</code> <code>int</code> <p>Maximum number of optimization trials to run. More trials        generally lead to better results but take longer.</p> <code>120</code> <code>replicates</code> <code>int</code> <p>Number of replicates per trial for statistical robustness.        Higher values reduce noise but increase computation time.</p> <code>1</code> <code>timeout</code> <code>Optional[float]</code> <p>Maximum time in seconds for the entire optimization process.     If None, optimization runs until max_trials is reached.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[TrialResult]</code> <p>List of TrialResult objects containing results from all completed trials,</p> <code>List[TrialResult]</code> <p>including both successful and failed trials with error information.</p>"},{"location":"reference/#octuner.optimization.optimizer.LLMOptimizer.get_best_parameters","title":"<code>get_best_parameters()</code>","text":"<p>Get the best parameter configuration found during optimization.</p> <p>This method returns the parameter values from the best trial according to the optimization strategy. The definition of \"best\" depends on the strategy used (e.g., highest quality for Pareto, lowest combined score for scalarized).</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary mapping parameter paths to their optimal values.</p> <code>Dict[str, Any]</code> <p>Returns empty dictionary if no trials completed successfully.</p>"},{"location":"reference/#octuner.optimization.optimizer.LLMOptimizer.get_best_trial","title":"<code>get_best_trial()</code>","text":"<p>Get the best trial result from the optimization.</p> <p>This method returns the complete TrialResult object for the best trial, including parameters, metrics, and success status. The best trial is determined by the optimization strategy used.</p> <p>Returns:</p> Type Description <code>Optional[TrialResult]</code> <p>TrialResult object for the best trial, or None if no trials</p> <code>Optional[TrialResult]</code> <p>completed successfully.</p>"},{"location":"reference/#optimizationstrategy","title":"OptimizationStrategy","text":"<p>Abstract base class for different optimization strategies (single objective, multi-objective).</p>"},{"location":"reference/#octuner.optimization.optimizer.OptimizationStrategy","title":"<code>octuner.optimization.optimizer.OptimizationStrategy(constraints=None, scalarization_weights=None)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for optimization strategies in Octuner.</p> <p>This class defines the interface that all optimization strategies must implement to work with the LLMOptimizer. It provides a unified way to handle different optimization approaches while maintaining compatibility with Optuna.</p> <p>Each strategy must implement methods for creating Optuna studies, computing objective values from metric results, and determining the best trial from completed studies.</p>"},{"location":"reference/#octuner.optimization.optimizer.OptimizationStrategy.create_study","title":"<code>create_study(study_name, seed=None)</code>  <code>abstractmethod</code>","text":"<p>Create an Optuna study configured for this optimization strategy.</p> <p>Parameters:</p> Name Type Description Default <code>study_name</code> <code>str</code> <p>Name for the Optuna study</p> required <code>seed</code> <code>Optional[int]</code> <p>Random seed for reproducible optimization</p> <code>None</code> <p>Returns:</p> Type Description <code>Study</code> <p>Configured Optuna study ready for optimization</p>"},{"location":"reference/#octuner.optimization.optimizer.OptimizationStrategy.compute_objectives","title":"<code>compute_objectives(result)</code>  <code>abstractmethod</code>","text":"<p>Convert the raw metric results (quality, cost, latency) into objective values that Optuna can optimize. The conversion depends on the specific strategy (e.g., Pareto uses multiple objectives, scalarized combines them into one).</p> <p>Parameters:</p> Name Type Description Default <code>result</code> <code>MetricResult</code> <p>MetricResult containing quality, cost, and latency</p> required <p>Returns:</p> Type Description <code>Tuple[float, ...]</code> <p>Tuple of objective values for Optuna optimization</p>"},{"location":"reference/#octuner.optimization.optimizer.OptimizationStrategy.get_fallback_objectives","title":"<code>get_fallback_objectives()</code>  <code>abstractmethod</code>","text":"<p>When a trial fails (e.g., due to parameter constraints or errors), this method provides objective values that represent the worst possible performance, ensuring failed trials are properly ranked.</p> <p>Returns:</p> Type Description <code>Tuple[float, ...]</code> <p>Tuple of fallback objective values</p>"},{"location":"reference/#octuner.optimization.optimizer.OptimizationStrategy.get_best_trial_from_study","title":"<code>get_best_trial_from_study(study)</code>  <code>abstractmethod</code>","text":"<p>Get the best trial from the study according to this strategy.</p> <p>Different strategies define \"best\" differently: - Pareto: Highest quality among Pareto-optimal solutions - Constrained: Highest quality among constraint-satisfying trials - Scalarized: Lowest combined objective score</p> <p>Parameters:</p> Name Type Description Default <code>study</code> <code>Study</code> <p>Completed Optuna study</p> required <p>Returns:</p> Type Description <code>Optional[FrozenTrial]</code> <p>Best trial according to this strategy, or None if no trials completed</p>"},{"location":"reference/#datasetexecutor","title":"DatasetExecutor","text":"<p>Executes trials on datasets with parallel processing support.</p>"},{"location":"reference/#octuner.optimization.executor.DatasetExecutor","title":"<code>octuner.optimization.executor.DatasetExecutor(component, entrypoint, dataset, metric, max_workers=1)</code>","text":"<p>DatasetExecutor is a core component of the optimization system that handles the execution of evaluation trials during parameter optimization. It manages the evaluation of components over datasets, collects performance metrics, and provides both sequential and parallel execution capabilities.</p> <p>How it works:</p> <ol> <li> <p>Parameter Application: Applies trial parameters to the component using    the parameter setter utilities, ensuring consistent parameter configuration    across trials.</p> </li> <li> <p>Dataset Evaluation: Executes the entrypoint function over each dataset    item, collecting outputs and computing quality scores using the provided    metric function.</p> </li> <li> <p>Metrics Collection: Automatically collects comprehensive metrics including    quality scores, execution costs, and latency measurements for each trial.</p> </li> <li> <p>Parallel Execution: Supports concurrent evaluation of dataset items    using ThreadPoolExecutor for faster trial execution when max_workers &gt; 1.</p> </li> <li> <p>Statistical Aggregation: Provides robust statistical aggregation across    replicates and dataset items using median-based aggregation for stability.</p> </li> </ol> <p>Key Features:</p> <ul> <li>Parallel Execution: Multi-threaded evaluation for faster optimization</li> <li>Comprehensive Metrics: Quality, cost, and latency tracking</li> <li>Statistical Robustness: Median-based aggregation to handle outliers</li> <li>Error Handling: Graceful handling of individual item failures</li> <li>Replicate Support: Multiple trial runs for statistical significance</li> <li>Cost Tracking: Automatic cost collection from tunable components</li> </ul> <p>Constructor</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>Any</code> <p>The component to evaluate. Must be tunable and support parameter         setting via the parameter setter utilities.</p> required <code>entrypoint</code> <code>EntrypointFunction</code> <p>Function that evaluates the component with input data.        Called as <code>entrypoint(component, input_data)</code> for each        dataset item. Should return a dictionary or object that        can be processed by the metric function.</p> required <code>dataset</code> <code>Dataset</code> <p>List of input/target pairs for evaluation. Each item should     be a dictionary with 'input' and 'target' keys containing     the input data and expected output respectively.</p> required <code>metric</code> <code>MetricFunction</code> <p>Function that computes quality scores from evaluation results.    Called as <code>metric(output, target)</code> where output is the result    from entrypoint and target is the expected output. Should    return a float score (higher is better).</p> required <code>max_workers</code> <code>int</code> <p>Maximum number of concurrent workers for parallel         evaluation. Use 1 for sequential execution, &gt;1 for         parallel execution. Higher values speed up I/O-bound         tasks but may not help with CPU-bound tasks due to         Python's GIL.</p> <code>1</code>"},{"location":"reference/#octuner.optimization.executor.DatasetExecutor.execute_trial","title":"<code>execute_trial(parameters)</code>","text":"<p>Execute a single evaluation trial with the given parameters. t applies the trial parameters to the component, executes the evaluation over all dataset items, and returns aggregated metrics including quality, cost, and latency.</p> <p>The execution process:</p> <ol> <li>Parameter Application: Sets the trial parameters on the component</li> <li>Call Log Clearing: Clears any previous call logs for clean metrics</li> <li>Dataset Evaluation: Runs the entrypoint function over each dataset item</li> <li>Metrics Collection: Collects quality scores, costs, and timing data</li> <li>Statistical Aggregation: Computes median quality and total cost</li> </ol> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, Any]</code> <p>Dictionary of parameter values to set on the component.       Keys should match the parameter paths from the search space.       Example: {\"llm.temperature\": 0.7, \"llm.max_tokens\": 100}</p> required <p>Returns:</p> Type Description <code>MetricResult</code> <p>MetricResult containing:</p> <code>MetricResult</code> <ul> <li>quality: Median quality score across all dataset items</li> </ul> <code>MetricResult</code> <ul> <li>cost: Total cost from all component calls (if available)</li> </ul> <code>MetricResult</code> <ul> <li>latency_ms: Total execution time in milliseconds</li> </ul>"},{"location":"reference/#octuner.optimization.executor.DatasetExecutor.execute_with_replicates","title":"<code>execute_with_replicates(parameters, replicates=1)</code>","text":"<p>Execute a trial multiple times and aggregate results for statistical robustness. It's particularly useful for optimization scenarios where individual trials may have high variance due to non-deterministic components or external factors.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, Any]</code> <p>Dictionary of parameter values to set on the component.       Same format as execute_trial().</p> required <code>replicates</code> <code>int</code> <p>Number of times to run the trial. Higher values provide        better statistical significance but take longer. Typical        values range from 1-10 depending on variance requirements.</p> <code>1</code> <p>Returns:</p> Type Description <code>MetricResult</code> <p>MetricResult containing aggregated metrics across all replicates:</p>"},{"location":"reference/#octuner.optimization.executor.DatasetExecutor.execute","title":"<code>execute(parameters)</code>","text":"<p>Execute the full dataset and return per-item results, unlike execute_trial() which returns aggregated metrics. It's useful for detailed analysis, debugging, r when you need to examine individual item performance rather than overall trial performance.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, Any]</code> <p>Dictionary of parameter values to set on the component.       Same format as execute_trial().</p> required <p>Returns:</p> Type Description <code>List[MetricResult]</code> <p>List of MetricResult objects, one for each dataset item. Each result</p> <code>List[MetricResult]</code> <p>contains the quality, cost, and latency for that specific item.</p>"},{"location":"reference/#discovery","title":"Discovery","text":""},{"location":"reference/#componentdiscovery","title":"ComponentDiscovery","text":"<p>Discovers tunable components in a component tree and builds search spaces.</p>"},{"location":"reference/#octuner.discovery.discovery.ComponentDiscovery","title":"<code>octuner.discovery.discovery.ComponentDiscovery(include_patterns=None, exclude_patterns=None)</code>","text":"<p>This is a core component of Octuner that automatically finds and catalogs all tunable parameters within an object hierarchy. It recursively traverses object attributes to identify components that implement the TunableMixin protocol, building a search space that can be optimized by the AutoTuner.</p> <p>How it works: 1. Recursive Traversal: Starting from a root component, it recursively explores     all object attributes using <code>__dict__</code> introspection, avoiding circular references     and method calls.</p> <ol> <li> <p>Tunable Detection: For each object found, it checks if it implements the     TunableMixin protocol using <code>is_llm_tunable()</code>, which supports both:         - Instance-based tunables (legacy): Objects with <code>get_tunable_parameters()</code>                                             method         - Registry-based tunables (recommended): Objects registered through                                                  <code>register_tunable_class()</code></p> </li> <li> <p>Parameter Extraction: For tunable objects, it extracts parameter definitions     including type, range, and default values using <code>get_tunable_parameters()</code>.</p> </li> <li> <p>Path Mapping: Each discovered parameter is mapped to a dotted path     (e.g., \"classifier_llm.temperature\") that uniquely identifies its location in     the hierarchy.</p> </li> <li> <p>Filtering: Optional include/exclude patterns can be used to focus the search     space on specific parameters or components.</p> </li> </ol> Example <pre><code>from octuner import ComponentDiscovery, MultiProviderTunableLLM\n\n# Create a complex component hierarchy\nanalyzer = TunableSentimentAnalyzer(config_file)\n\n# Discover all tunable parameters\ndiscovery = ComponentDiscovery()\ntunables = discovery.discover(analyzer)\n\n# Result: {\n#     \"classifier_llm\": {\n#         \"temperature\": (\"float\", 0.0, 2.0),\n#         \"max_tokens\": (\"int\", 64, 4096),\n#         \"provider_model\": (\"choice\", [\"openai:gpt-4\", \"gemini:gemini-pro\"])\n#     },\n#     \"confidence_llm\": { ... },\n#     \"reasoning_llm\": { ... }\n# }\n\n# Focus on specific parameters\nfocused_discovery = ComponentDiscovery(\n    include_patterns=[\"*.temperature\", \"*.provider_model\"],\n    exclude_patterns=[\"*.verbose\"]\n)\nfocused_tunables = focused_discovery.discover(analyzer)\n</code></pre> Integration with AutoTuner <p>ComponentDiscovery is automatically used by AutoTuner to build the search space before optimization begins. The discovered parameters become the dimensions that  the optimizer explores to find the best configuration.</p> <p>Attributes:</p> Name Type Description <code>include_patterns</code> <p>List of glob patterns to include in discovery</p> <code>exclude_patterns</code> <p>List of glob patterns to exclude from discovery</p> <p>Initialize discovery with optional filters.</p> <p>Parameters:</p> Name Type Description Default <code>include_patterns</code> <code>Optional[List[str]]</code> <p>Glob patterns to include (e.g., [\"*.temperature\"])</p> <code>None</code> <code>exclude_patterns</code> <code>Optional[List[str]]</code> <p>Glob patterns to exclude (e.g., [\"*.verbose\"])</p> <code>None</code>"},{"location":"reference/#octuner.discovery.discovery.ComponentDiscovery.discover","title":"<code>discover(component)</code>","text":"<p>Discover all tunable components in the component tree.</p> <p>This is the main entry point for component discovery. It performs a recursive traversal of the component hierarchy, identifying all objects that implement the TunableMixin protocol and extracting their tunable parameters.</p> The discovery process <ol> <li>Starts from the provided root component</li> <li>Recursively explores all object attributes</li> <li>Identifies tunable components using <code>is_llm_tunable()</code></li> <li>Extracts parameter definitions from each tunable component</li> <li>Applies \"include/exclude\" filters (optional)</li> <li>Returns a structured mapping of paths to parameters</li> </ol> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>Any</code> <p>Root component to search. Can be any Python object, but typically      a component hierarchy containing multiple tunable LLMs or other      tunable components.</p> required <p>Returns:</p> Name Type Description <code>Dict[str, Dict[str, Tuple[ParamType, Any, Any]]]</code> <p>Dictionary mapping dotted paths to tunable parameter definitions. The structure is:</p> <code>Dict[str, Dict[str, Tuple[ParamType, Any, Any]]]</code> <p>```</p> <code>Dict[str, Dict[str, Tuple[ParamType, Any, Any]]]</code> <p>{ \"component_path\": {     \"param_name\": (param_type, min_value, max_value),     \"another_param\": (param_type, choices_or_default, ...) }</p> <code>Dict[str, Dict[str, Tuple[ParamType, Any, Any]]]</code> <p>}</p> <code>Dict[str, Dict[str, Tuple[ParamType, Any, Any]]]</code> <p>```</p> <code>Where</code> <code>Dict[str, Dict[str, Tuple[ParamType, Any, Any]]]</code> <code>Dict[str, Dict[str, Tuple[ParamType, Any, Any]]]</code> <ul> <li><code>component_path</code>: Dotted path to the component (e.g., \"classifier_llm\")</li> </ul> <code>Dict[str, Dict[str, Tuple[ParamType, Any, Any]]]</code> <ul> <li><code>param_name</code>: Name of the tunable parameter</li> </ul> <code>Dict[str, Dict[str, Tuple[ParamType, Any, Any]]]</code> <ul> <li><code>param_type</code>: Type of parameter (\"float\", \"int\", \"choice\", \"bool\")</li> </ul> <code>Dict[str, Dict[str, Tuple[ParamType, Any, Any]]]</code> <ul> <li><code>min_value</code>, <code>max_value</code>: Range for numeric parameters</li> </ul> <code>Dict[str, Dict[str, Tuple[ParamType, Any, Any]]]</code> <ul> <li><code>choices_or_default</code>: Choices for choice parameters or default values</li> </ul> Note <p>I suppose this method is safe to call on any object. It will return an empty dictionary if no tunable components are found.</p>"},{"location":"reference/#discovery-functions","title":"Discovery Functions","text":""},{"location":"reference/#octuner.discovery.discovery.build_search_space","title":"<code>octuner.discovery.discovery.build_search_space(discovered)</code>","text":"<p>This method transforms the hierarchical discovery results into a flat search space suitable for optimization. It converts the nested structure from ComponentDiscovery into a flat dictionary where each parameter is identified by its full dotted path.</p> <p>The transformation: - Input: <code>{\"component\": {\"param\": (type, min, max)}}</code> - Output: <code>{\"component.param\": (type, min, max)}</code></p> <p>This flattened structure is what the optimizer uses to define the search space dimensions for parameter optimization.</p> <p>Parameters:</p> Name Type Description Default <code>discovered</code> <code>Dict[str, Dict[str, Tuple[ParamType, Any, Any]]]</code> <p>Discovery results from ComponentDiscovery.discover() or        discover_tunable_components(). Dictionary mapping component        paths to their tunable parameters.</p> required <p>Returns:</p> Type Description <code>Dict[str, Tuple[ParamType, Any, Any]]</code> <p>Dictionary mapping full parameter paths to parameter definitions.</p> <code>Dict[str, Tuple[ParamType, Any, Any]]</code> <p>Each key is a dotted path like \"classifier_llm.temperature\" and each</p> <code>Dict[str, Tuple[ParamType, Any, Any]]</code> <p>value is a tuple containing the parameter type and constraints.</p>"},{"location":"reference/#configuration","title":"Configuration","text":""},{"location":"reference/#configloader","title":"ConfigLoader","text":"<p>Loads and validates YAML configuration files for LLM providers and models.</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader","title":"<code>octuner.config.loader.ConfigLoader(config_file)</code>","text":"<p>This class provides utilities to load YAML configuration files as described in config_templates/*.yaml.</p> <p>It allows to get available providers, models, parameters, pricing, and capabilities. Those capabilities become available to the tuning algorithms to know what parameters can be optimized, their ranges, types, and default values.</p> <p>IMPORTANT: Note that when instantiating this class, the configuration file is loaded immediately.</p> <p>Initialize config loader with a specific file.</p> <p>Parameters:</p> Name Type Description Default <code>config_file</code> <code>str</code> <p>Path to the YAML configuration file</p> required"},{"location":"reference/#octuner.config.loader.ConfigLoader.get_providers","title":"<code>get_providers()</code>","text":"<p>Get list of available providers.</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader.get_provider_config","title":"<code>get_provider_config(provider_name)</code>","text":"<p>Get configuration for a specific provider.</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader.get_default_model","title":"<code>get_default_model(provider_name)</code>","text":"<p>Get default model for a provider.</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader.get_available_models","title":"<code>get_available_models(provider_name)</code>","text":"<p>Get available models for a provider.</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader.get_pricing","title":"<code>get_pricing(provider_name, model)</code>","text":"<p>Get pricing for a model (input_cost, output_cost per 1M tokens).</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader.get_model_capabilities","title":"<code>get_model_capabilities(provider_name, model)</code>","text":"<p>Get capabilities for a specific model.</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader.get_supported_parameters","title":"<code>get_supported_parameters(provider_name, model)</code>","text":"<p>Get list of parameters that can be optimized for a model.</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader.model_supports_parameter","title":"<code>model_supports_parameter(provider_name, model, parameter)</code>","text":"<p>Check if a model supports a specific parameter.</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader.get_parameter_range","title":"<code>get_parameter_range(provider_name, model, parameter)</code>","text":"<p>Get optimization range for a parameter.</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader.get_parameter_default","title":"<code>get_parameter_default(provider_name, model, parameter)</code>","text":"<p>Get default value for a parameter.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider</p> required <code>model</code> <code>str</code> <p>Name of the model</p> required <code>parameter</code> <code>str</code> <p>Name of the parameter</p> required <p>Returns:</p> Type Description <p>Default parameter value from configuration</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameter default is not defined in configuration</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader.get_parameter_type","title":"<code>get_parameter_type(provider_name, model, parameter)</code>","text":"<p>Get the expected type for a parameter from YAML configuration.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider</p> required <code>model</code> <code>str</code> <p>Name of the model</p> required <code>parameter</code> <code>str</code> <p>Name of the parameter</p> required <p>Returns:</p> Type Description <code>str</code> <p>Parameter type ('int', 'float', 'str', 'bool', 'choice')</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameter type is not defined in configuration</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader.get_forced_parameter","title":"<code>get_forced_parameter(provider_name, model, parameter)</code>","text":"<p>Get forced value for a parameter (if any).</p>"},{"location":"reference/#octuner.config.loader.ConfigLoader.validate_config","title":"<code>validate_config()</code>","text":"<p>Validate the configuration structure.</p>"},{"location":"reference/#utilities","title":"Utilities","text":""},{"location":"reference/#exporter-functions","title":"Exporter Functions","text":"<p>Functions for saving and loading optimized parameters.</p>"},{"location":"reference/#octuner.utils.exporter.save_parameters_to_yaml","title":"<code>octuner.utils.exporter.save_parameters_to_yaml(parameters, path, metadata=None)</code>","text":"<p>Save parameters to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>parameters</code> <code>Dict[str, Any]</code> <p>Dictionary of parameter values</p> required <code>path</code> <code>str</code> <p>Path to save the YAML file</p> required <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Optional metadata to include</p> <code>None</code>"},{"location":"reference/#octuner.utils.exporter.load_parameters_from_yaml","title":"<code>octuner.utils.exporter.load_parameters_from_yaml(path)</code>","text":"<p>Load parameters from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the YAML file</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary of parameter values</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file doesn't exist</p> <code>YAMLError</code> <p>If the file is invalid YAML</p>"},{"location":"reference/#octuner.utils.exporter.create_metadata_summary","title":"<code>octuner.utils.exporter.create_metadata_summary(trials, optimization_mode='pareto', dataset_size=0, total_trials=0, best_quality=0.0, best_cost=None, best_latency_ms=None, dataset_fingerprint=None)</code>","text":"<p>Create a metadata summary for the optimization results.</p> <p>Parameters:</p> Name Type Description Default <code>trials</code> <code>list</code> <p>List of trial results</p> required <code>optimization_mode</code> <code>str</code> <p>Mode used for optimization</p> <code>'pareto'</code> <code>dataset_size</code> <code>int</code> <p>Number of examples in the dataset</p> <code>0</code> <code>total_trials</code> <code>int</code> <p>Total number of trials run</p> <code>0</code> <code>best_quality</code> <code>float</code> <p>Best quality score achieved</p> <code>0.0</code> <code>best_cost</code> <code>Optional[float]</code> <p>Best cost achieved (if available)</p> <code>None</code> <code>best_latency_ms</code> <code>Optional[float]</code> <p>Best latency achieved (if available)</p> <code>None</code> <code>dataset_fingerprint</code> <code>Optional[str]</code> <p>Dataset fingerprint (if available)</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary of metadata</p>"},{"location":"reference/#parameter-setter","title":"Parameter Setter","text":"<p>Apply optimized parameters to components.</p>"},{"location":"reference/#octuner.utils.setter.set_parameters","title":"<code>octuner.utils.setter.set_parameters(component, parameters, strict=False)</code>","text":"<p>Convenience function to set parameters on a component.</p> <p>Parameters:</p> Name Type Description Default <code>component</code> <code>Any</code> <p>Component to set parameters on</p> required <code>parameters</code> <code>Dict[str, Any]</code> <p>Dictionary mapping dotted paths to values</p> required <code>strict</code> <code>bool</code> <p>If True, raise exceptions instead of logging warnings</p> <code>False</code>"},{"location":"reference/#type-definitions","title":"Type Definitions","text":""},{"location":"reference/#core-types","title":"Core Types","text":"<p>Type definitions and data classes used throughout Octuner.</p>"},{"location":"reference/#octuner.tunable.types.ParamType","title":"<code>octuner.tunable.types.ParamType = Literal['float', 'int', 'choice', 'bool']</code>  <code>module-attribute</code>","text":""},{"location":"reference/#octuner.tunable.types.Dataset","title":"<code>octuner.tunable.types.Dataset = List[DatasetItem]</code>  <code>module-attribute</code>","text":""},{"location":"reference/#octuner.tunable.types.MetricResult","title":"<code>octuner.tunable.types.MetricResult(quality, cost=None, latency_ms=None)</code>  <code>dataclass</code>","text":"<p>Result of a single metric evaluation.</p>"},{"location":"reference/#octuner.tunable.types.TrialResult","title":"<code>octuner.tunable.types.TrialResult(trial_number, parameters, metrics, success=True, error=None)</code>  <code>dataclass</code>","text":"<p>Result of a single optimization trial.</p>"},{"location":"reference/#octuner.tunable.types.SearchResult","title":"<code>octuner.tunable.types.SearchResult(best_trial, all_trials, optimization_mode, dataset_size, total_trials, best_parameters, metrics_summary)</code>  <code>dataclass</code>","text":"<p>Result of an optimization search.</p>"},{"location":"reference/#octuner.tunable.types.SearchResult.save_best","title":"<code>save_best(path)</code>","text":"<p>Save best parameters to YAML file.</p>"},{"location":"reference/#octuner.tunable.types.Constraints","title":"<code>octuner.tunable.types.Constraints = Dict[str, Union[float, int]]</code>  <code>module-attribute</code>","text":""},{"location":"reference/#octuner.tunable.types.ScalarizationWeights","title":"<code>octuner.tunable.types.ScalarizationWeights(cost_weight=1.0, latency_weight=1.0)</code>  <code>dataclass</code>","text":"<p>Weights for scalarized optimization mode.</p>"}]}